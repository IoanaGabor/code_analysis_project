{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdhGW1MxQjnz",
        "outputId": "8a1bed31-fb7a-47c2-e1ca-e4d8f21507b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (1.9.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install polars\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "\n",
        "splits = {\n",
        "    'train': 'hf://datasets/blastwind/deprecated-github-code-haskell-function/data/train-*-of-*.parquet',\n",
        "    'test': 'hf://datasets/blastwind/deprecated-github-code-haskell-function/data/test-*-of-*.parquet',\n",
        "    'valid': 'hf://datasets/blastwind/deprecated-github-code-haskell-function/data/valid-00000-of-00001-636cb804972d8982.parquet'\n",
        "}\n",
        "\n",
        "\n",
        "df = pl.read_parquet(splits['train'])\n",
        "\n"
      ],
      "metadata": {
        "id": "3WsHzUqBVvnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataframe = df.to_pandas()\n",
        "\n",
        "dataframe = dataframe.loc[\n",
        "    (dataframe[\"is_commented\"] == True) & (dataframe[\"n_ast_nodes\"] < 50),\n",
        "    [\"full_code\", \"uncommented_code\", \"function_only_code\"]\n",
        "]\n",
        "\n",
        "\n",
        "dataframe[\"comments\"] = dataframe.apply(\n",
        "    lambda row: row[\"full_code\"].replace(row[\"uncommented_code\"], \"\").strip(), axis=1\n",
        ")\n",
        "\n",
        "dataframe = dataframe.head(10000)\n",
        "\n",
        "dataframe.to_csv('data-small.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "I14IsBN4ep0Z",
        "outputId": "7066b295-021e-47b1-81cc-c2101ada0702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-337b54f295e0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m dataframe = dataframe.loc[\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"is_commented\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_ast_nodes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m\"full_code\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"uncommented_code\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"function_only_code\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "def tokenize_and_build_vocab(column):\n",
        "    vocab_counter = Counter()\n",
        "    for text in column:\n",
        "        tokens = [token.text for token in nlp(text.lower())]\n",
        "        vocab_counter.update(tokens)\n",
        "    return vocab_counter\n",
        "\n",
        "\n",
        "vocab_counter = tokenize_and_build_vocab(dataframe[\"comments\"])\n",
        "\n",
        "vocab = {token: idx for idx, (token, _) in enumerate(vocab_counter.items(), start=4)}  # Starting at index 4 for special tokens\n",
        "vocab['<unk>'] = 0\n",
        "vocab['<pad>'] = 1\n",
        "vocab['<bos>'] = 2\n",
        "vocab['<eos>'] = 3\n",
        "\n",
        "def text_to_ids(text, vocab):\n",
        "\n",
        "    tokens = [token.text for token in nlp(text.lower())]\n",
        "\n",
        "    return [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
        "\n",
        "text = dataframe[\"comments\"]\n",
        "token_ids = text_to_ids(text, vocab)\n",
        "\n",
        "print(f\"Text: {text}\")\n",
        "print(f\"Token IDs: {token_ids}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "SGNGJxukEXnU",
        "outputId": "dc1c762f-0d17-4814-f7c9-36c961aab546"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-69743705a624>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# set library-specific custom warning handling before doing anything else\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup_default_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msetup_default_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/errors.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# fmt: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVARIABLE_RE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfigValidationError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPromise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_cupy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhas_cupy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdlpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2127\u001b[0m \u001b[0;31m# quantization depends on torch.fx and torch.ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m \u001b[0;31m# Import quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2129\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantization\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mquantization\u001b[0m  \u001b[0;31m# usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m \u001b[0;31m# Import the quasi random sampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/quantization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# mypy: allow-untyped-defs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfake_quantize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfuse_modules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfuse_modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfuser_method_mappings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mobserver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/quantization/fake_quantize.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \"\"\"\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from torch.ao.quantization.fake_quantize import (\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0m_is_fake_quant_script_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0m_is_per_channel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfake_quantize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfuse_modules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfuse_modules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuse_modules_qat\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfuser_method_mappings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/fake_quantize.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from torch.ao.quantization.observer import (\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0m_with_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdefault_fixed_qparams_range_0to1_observer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from torch.ao.quantization.utils import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mcalculate_qmin_qmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mcheck_min_max_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_type\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuantType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparametrize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_parametrized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m '''\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgraph_module\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_symbolic_trace\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msymbolic_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTracer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProxyableClassMeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCodeGen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_compatibility\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_custom_builtins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_is_from_torch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_PyTreeCodeGen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPythonCode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m __all__ = [\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcompatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_backward_compatible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m     \"\"\"\n\u001b[1;32m    830\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmain\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mstructure\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mFX\u001b[0m \u001b[0mIntermediate\u001b[0m \u001b[0mRepresentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py\u001b[0m in \u001b[0;36mGraph\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcompatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_backward_compatible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mnode_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_transform\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Argument'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m         \"\"\"\n\u001b[1;32m   1337\u001b[0m         \u001b[0mCopy\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mone\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0minto\u001b[0m \u001b[0manother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0marg_transform\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mneeds\u001b[0m \u001b[0mto\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0marguments\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/_compatibility.py\u001b[0m in \u001b[0;36mmark_back_compat\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     17\u001b[0m \"\"\"\n\u001b[1;32m     18\u001b[0m             \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0m_BACK_COMPAT_OBJECTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0m_MARKED_WITH_COMPATIBILITY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "class CustomTokenizer:\n",
        "    def __init__(self, oov_token=\"<UNK>\"):\n",
        "        \"\"\"\n",
        "        Initializes the tokenizer with empty mappings and an OOV (out-of-vocabulary) token.\n",
        "        \"\"\"\n",
        "        self.word_to_id = {}\n",
        "        self.id_to_word = {}\n",
        "        self.oov_token = oov_token\n",
        "\n",
        "        self.word_to_id[oov_token] = 0\n",
        "        self.word_to_id[\"<start>\"] = 1;\n",
        "        self.word_to_id[\"<end>\"] = 2;\n",
        "        self.id_to_word[0] = oov_token\n",
        "        self.id_to_word[1] = \"<start>\"\n",
        "        self.id_to_word[2] = \"<end>\"\n",
        "\n",
        "    def load(self, word_list):\n",
        "        \"\"\"\n",
        "        Dynamically loads a list of words into the tokenizer and assigns free IDs.\n",
        "\n",
        "        Args:\n",
        "            word_list (list of str): The list of words to add to the tokenizer.\n",
        "        \"\"\"\n",
        "        # Ensure all words are lowercase\n",
        "        word_list = [word.lower() for word in word_list]\n",
        "\n",
        "        # Start assigning IDs from the current maximum ID + 1\n",
        "        next_id = max(self.word_to_id.values()) + 1\n",
        "\n",
        "        for word in word_list:\n",
        "            if word not in self.word_to_id:\n",
        "                self.word_to_id[word] = next_id\n",
        "                self.id_to_word[next_id] = word\n",
        "                next_id += 1\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"\n",
        "        Encodes a string into a list of IDs.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text to encode.\n",
        "\n",
        "        Returns:\n",
        "            list of int: List of word IDs.\n",
        "        \"\"\"\n",
        "        words = text.lower().split()  # Convert text to lowercase and split into words\n",
        "        return [self.word_to_id.get(word, self.word_to_id[self.oov_token]) for word in words]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"\n",
        "        Decodes a list of IDs into a string.\n",
        "\n",
        "        Args:\n",
        "            ids (list of int): List of word IDs to decode.\n",
        "\n",
        "        Returns:\n",
        "            str: The decoded string.\n",
        "        \"\"\"\n",
        "        return ' '.join(self.id_to_word.get(i, self.oov_token) for i in ids)\n",
        "\n",
        "    def export(self, filename):\n",
        "        \"\"\"\n",
        "        Exports the word_to_id and id_to_word mappings to a binary file.\n",
        "\n",
        "        Args:\n",
        "            filename (str): The path of the file to save the mappings.\n",
        "        \"\"\"\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump((self.word_to_id, self.id_to_word), f)\n",
        "\n",
        "    def import_mappings(self, filename):\n",
        "        \"\"\"\n",
        "        Loads word_to_id and id_to_word mappings from a binary file.\n",
        "\n",
        "        Args:\n",
        "            filename (str): The path of the file to load the mappings from.\n",
        "        \"\"\"\n",
        "        with open(filename, 'rb') as f:\n",
        "            self.word_to_id, self.id_to_word = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "xnMqL9j_UZLp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "rdGhKxbng1-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "ZzhICbXemOKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataframe = pd.read_csv('data-small.csv')\n",
        "tokenizer = CustomTokenizer()\n",
        "dataframe = dataframe.head(10)\n",
        "tokenizer = CustomTokenizer()\n",
        "for comment in dataframe[\"comments\"]:\n",
        "  tokenizer.load(comment.lower().split())"
      ],
      "metadata": {
        "id": "QUjiBGukUDF0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98_gRPuEhzTI",
        "outputId": "67393cde-bcdf-46f7-ea1b-6c260a7b697d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tree_sitter==0.23.0\n",
        "!pip install tree_sitter_haskell"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5QUCsAKihb-",
        "outputId": "9cd11921-1cc3-4d7d-cfcd-e5faa475a0ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tree_sitter==0.23.0 in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: tree_sitter_haskell in /usr/local/lib/python3.10/dist-packages (0.23.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tree_sitter_haskell as tshaskell\n",
        "import numpy as np\n",
        "from tree_sitter import Language, Parser\n",
        "\n",
        "\n",
        "HS_LANGUAGE = Language(tshaskell.language())\n",
        "parser = Parser(HS_LANGUAGE)\n"
      ],
      "metadata": {
        "id": "XNf9tk_JjQRH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_tree_features(code):\n",
        "    try:\n",
        "        example_bytes = code.encode()\n",
        "        tree = parser.parse(example_bytes)\n",
        "        root_node = tree.root_node\n",
        "\n",
        "        def extract_features(node, parent_index=None, nodes=[], edges=[]):\n",
        "            node_type = node.type\n",
        "            start_pos = node.start_point\n",
        "            end_pos = node.end_point\n",
        "            code_value = code[node.start_byte:node.end_byte]\n",
        "            tokenizer.load(node_type.lower().split())\n",
        "            feature_vector = [node_type]\n",
        "            node_index = len(nodes)\n",
        "            nodes.append(feature_vector)\n",
        "\n",
        "            if parent_index is not None:\n",
        "                edges.append((parent_index, node_index))\n",
        "\n",
        "            for child in node.children:\n",
        "                extract_features(child, parent_index=node_index, nodes=nodes, edges=edges)\n",
        "\n",
        "            return nodes, edges\n",
        "\n",
        "        nodes, edge_list = extract_features(root_node)\n",
        "        code_value = code[root_node.start_byte:root_node.end_byte]\n",
        "        tokenizer.load(code_value.lower().split())\n",
        "        num_nodes = len(nodes)\n",
        "        adj_matrix = np.zeros((num_nodes, num_nodes), dtype=np.float32)\n",
        "        for parent, child in edge_list:\n",
        "            adj_matrix[parent, child] = 1\n",
        "\n",
        "        return nodes, adj_matrix\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing code: {e}\")\n",
        "        return None, None\n",
        "dataframe[\"nodes\"], dataframe[\"adjacency_matrix\"] = zip(*dataframe[\"function_only_code\"].apply(extract_tree_features))\n"
      ],
      "metadata": {
        "id": "g9xsJdoSjmAA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataframe.to_csv('data-ast-small.csv', index=False)\n",
        "#print(dataframe[\"nodes\"].size)\n",
        "#tokenizer.export(\"tokens.bin\")\n",
        "import pandas as pd\n",
        "dataframe = pd.read_csv('data-ast-small.csv')\n",
        "tokenizer = CustomTokenizer()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "bxAQoO4GqcsO",
        "outputId": "cf836348-8dc4-412c-bc8b-2dc20dd43297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data-ast-small.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cc17fd810c8c>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#tokenizer.export(\"tokens.bin\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data-ast-small.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_mappings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokens.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data-ast-small.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smvAVY58KWAl",
        "outputId": "9651c681-3a37-46ab-94b8-e25914d1db8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXGNXmBEtjy_",
        "outputId": "d18f4ed4-2c57-4500-aefe-9fe5cfde8e39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.67.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Layer\n",
        "from keras import activations\n",
        "import keras.backend as K\n",
        "\n",
        "class GCNLayer(Layer):\n",
        "    def __init__(self, units, activation='relu', initializer='glorot_uniform', sparse=False, use_bias=True, **kwargs):\n",
        "        self.activation = activations.get(activation)\n",
        "        self.output_dim = units\n",
        "        self.initializer = initializer\n",
        "        self.sparse = sparse\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        super(GCNLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(name='kernel',\n",
        "                                          shape=(input_shape[0][-1], self.output_dim),\n",
        "                                          initializer=self.initializer,\n",
        "                                          trainable=True)\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(name='bias',\n",
        "                                              shape=(self.output_dim,),\n",
        "                                              initializer='zeros',\n",
        "                                              trainable=True)\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "        super(GCNLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        assert isinstance(x, list)\n",
        "        nodes, edges = x\n",
        "        identity = tf.eye(tf.shape(edges)[-1], batch_shape=[tf.shape(edges)[0]], dtype=edges.dtype)\n",
        "        edges = edges + identity\n",
        "        output = tf.matmul(edges,nodes)\n",
        "        output = tf.matmul(output, self.kernel)\n",
        "\n",
        "        if self.use_bias:\n",
        "            output += self.bias\n",
        "\n",
        "        return self.activation(output)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        return (None,input_shape[0][1], self.output_dim)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'units': self.output_dim,\n",
        "            'activation': activations.serialize(self.activation),\n",
        "        }\n",
        "\n",
        "        base_config = super(GCNLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "metadata": {
        "id": "qImh_MqWthex"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import keras.utils\n",
        "from keras.layers import Input, Dense, Embedding, Activation, concatenate, Flatten, GRU, TimeDistributed, dot\n",
        "from keras.models import Model\n",
        "\n",
        "class CodeGNNGRU:\n",
        "    def __init__(self, config):\n",
        "        config['modeltype'] = 'codegnngru'\n",
        "\n",
        "        self.config = config\n",
        "        self.tdatvocabsize = config['tdatvocabsize']\n",
        "        self.comvocabsize = config['comvocabsize']\n",
        "        self.smlvocabsize = config['smlvocabsize']\n",
        "        self.tdatlen = config['tdatlen']\n",
        "        self.comlen = config['comlen']\n",
        "        self.smllen = config['maxastnodes']\n",
        "\n",
        "        self.config['batch_maker'] = 'graph_multi_1'\n",
        "\n",
        "        self.embdims = 100\n",
        "        self.smldims = 256\n",
        "        self.recdims = 256\n",
        "        self.tdddims = 256\n",
        "\n",
        "    def create_model(self):\n",
        "\n",
        "        tdat_input = Input(shape=(self.tdatlen,))\n",
        "        com_input = Input(shape=(self.comlen,))\n",
        "        node_input = Input(shape=(self.smllen,))\n",
        "        edge_input = Input(shape=(self.smllen, self.smllen))\n",
        "\n",
        "        tdel = Embedding(output_dim=self.embdims, input_dim=self.tdatvocabsize, mask_zero=False)\n",
        "        tde = tdel(tdat_input)\n",
        "\n",
        "        se = tdel(node_input)\n",
        "\n",
        "        tenc = GRU(self.recdims, return_state=True, return_sequences=True)\n",
        "        tencout, tstate_h = tenc(tde)\n",
        "\n",
        "        de = Embedding(output_dim=self.embdims, input_dim=self.comvocabsize, mask_zero=False)(com_input)\n",
        "        dec = GRU(self.recdims, return_sequences=True)\n",
        "        decout = dec(de, initial_state=tstate_h)\n",
        "\n",
        "        tattn = dot([decout, tencout], axes=[2, 2])\n",
        "        tattn = Activation('softmax')(tattn)\n",
        "        tcontext = dot([tattn, tencout], axes=[2, 1])\n",
        "\n",
        "        astwork = se\n",
        "\n",
        "        for i in range(self.config['asthops']):\n",
        "            astwork = GCNLayer(100)([astwork, edge_input])\n",
        "\n",
        "        astwork = GRU(self.recdims, return_sequences=True)(astwork, initial_state=tstate_h)\n",
        "\n",
        "        aattn = dot([decout, astwork], axes=[2, 2])\n",
        "        aattn = Activation('softmax')(aattn)\n",
        "        acontext = dot([aattn, astwork], axes=[2, 1])\n",
        "\n",
        "        context = concatenate([tcontext, decout, acontext])\n",
        "\n",
        "        out = TimeDistributed(Dense(self.tdddims, activation=\"relu\"))(context)\n",
        "\n",
        "        out = Flatten()(out)\n",
        "        out1 = Dense(self.comvocabsize, activation=\"softmax\")(out)\n",
        "\n",
        "        model = Model(inputs=[tdat_input, com_input, node_input, edge_input], outputs=out1)\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])\n",
        "        return self.config, model\n"
      ],
      "metadata": {
        "id": "BWrWF8f6txTV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.utils import to_categorical\n",
        "import ast\n",
        "\n",
        "config = {\n",
        "    'tdatvocabsize': 1000,\n",
        "    'comvocabsize': 1000,\n",
        "    'smlvocabsize':1000,\n",
        "    'tdatlen': 200,\n",
        "    'comlen': 500,\n",
        "    'maxastnodes': 70,\n",
        "    'asthops': 3,\n",
        "}\n",
        "model_instance = CodeGNNGRU(config)\n",
        "config, model = model_instance.create_model()\n",
        "\n",
        "tdat_input = np.zeros((len(dataframe) * config['comlen'], config['tdatlen']), dtype=np.float32)\n",
        "com_input = np.zeros((len(dataframe) * config['comlen'], config['comlen']), dtype=np.float32)\n",
        "node_input = np.zeros((len(dataframe) * config['comlen'], config['maxastnodes']), dtype=np.float32)\n",
        "edge_input = np.zeros((len(dataframe) * config['comlen'], config['maxastnodes'], config['maxastnodes']), dtype=np.float32)\n",
        "target_output = np.zeros((len(dataframe) * config['comlen'], config['comvocabsize']), dtype=np.float32)\n",
        "sample_idx = 0\n",
        "\n",
        "for i, (nodes, adj_matrix, tdat_text, com_text) in enumerate(zip(dataframe[\"nodes\"], dataframe[\"adjacency_matrix\"], dataframe[\"function_only_code\"],dataframe[\"comments\"])):\n",
        "    tdat_encoded = tokenizer.encode(tdat_text)\n",
        "    com_encoded = [1] + tokenizer.encode(com_text) + [2]\n",
        "    num_nodes = min(len(nodes), int(config['maxastnodes']))\n",
        "    node_encoded = []\n",
        "    for node in nodes[:num_nodes]:\n",
        "        encoded_node = tokenizer.encode(node[0])\n",
        "        if len(encoded_node) > 1:\n",
        "          node_encoded.append(encoded_node[0])\n",
        "\n",
        "    for j in range(len(com_encoded)):\n",
        "        target_output[sample_idx, com_encoded[j]] = 1\n",
        "        tdat_input[sample_idx, :len(tdat_encoded)] = tdat_encoded[:config[\"tdatlen\"]]\n",
        "        com_input[sample_idx, :len(com_encoded)] = com_encoded[:config[\"comlen\"]]\n",
        "        node_input[sample_idx, :len(node_encoded)] = node_encoded[:num_nodes]\n",
        "        edge_input[sample_idx, :num_nodes, :num_nodes] = adj_matrix[:num_nodes, :num_nodes]\n",
        "        sample_idx +=1\n",
        "\n",
        "print(tdat_input.shape)\n",
        "print(com_input.shape)\n",
        "print(node_input.shape)\n",
        "print(edge_input.shape)\n",
        "print(target_output.shape)\n",
        "print(tdat_input)\n",
        "print(com_input)\n",
        "print(node_input)\n",
        "print(edge_input)\n",
        "print(target_output)\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    [tdat_input, com_input, node_input, edge_input],\n",
        "    target_output,\n",
        "    epochs=5\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGMm76R9uLfQ",
        "outputId": "3233c4d4-d3dd-4823-d921-3f55942e1829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 200)\n",
            "(5000, 500)\n",
            "(5000, 70)\n",
            "(5000, 70, 70)\n",
            "(5000, 1000)\n",
            "[[136.   6. 137. ...   0.   0.   0.]\n",
            " [136.   6. 137. ...   0.   0.   0.]\n",
            " [136.   6. 137. ...   0.   0.   0.]\n",
            " ...\n",
            " [  0.   0.   0. ...   0.   0.   0.]\n",
            " [  0.   0.   0. ...   0.   0.   0.]\n",
            " [  0.   0.   0. ...   0.   0.   0.]]\n",
            "[[1. 3. 4. ... 0. 0. 0.]\n",
            " [1. 3. 4. ... 0. 0. 0.]\n",
            " [1. 3. 4. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 1. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 1. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 1. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "[[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Epoch 1/5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('code_gnn_model.keras')\n",
        "print(\"Model saved to 'code_gnn_model.h5'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09t1NKlSvSZF",
        "outputId": "88a51090-551d-489e-a35a-930eb2cf2f4b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to 'code_gnn_model.h5'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_commentary_greedy(model, tokenizer, tdat_encoded, node_encoded, edge_matrix, config, max_length=20):\n",
        "    \"\"\"\n",
        "    Predict commentary for given input in a greedy manner.\n",
        "\n",
        "    Args:\n",
        "        model: The trained CodeGNNGRU model.\n",
        "        tokenizer: Tokenizer to encode/decode text.\n",
        "        tdat_encoded: Encoded representation of the function code.\n",
        "        node_encoded: Encoded representation of the AST nodes.\n",
        "        edge_matrix: Adjacency matrix of the AST.\n",
        "        config: Configuration dictionary containing lengths and vocab sizes.\n",
        "        max_length: Maximum length of the generated commentary.\n",
        "\n",
        "    Returns:\n",
        "        Generated commentary string.\n",
        "    \"\"\"\n",
        "    # Initialize inputs\n",
        "    tdat_input = np.zeros((1, config['tdatlen']), dtype=np.float32)\n",
        "    node_input = np.zeros((1, config['maxastnodes']), dtype=np.float32)\n",
        "    edge_input = np.zeros((1, config['maxastnodes'], config['maxastnodes']), dtype=np.float32)\n",
        "    com_input = np.zeros((1, config['comlen']), dtype=np.float32)\n",
        "\n",
        "    # Populate input tensors\n",
        "    tdat_input[0, :len(tdat_encoded)] = tdat_encoded[:config['tdatlen']]\n",
        "    node_input[0, :len(node_encoded)] = node_encoded[:config['maxastnodes']]\n",
        "    edge_input[0, :len(edge_matrix), :len(edge_matrix)] = edge_matrix[:config['maxastnodes'], :config['maxastnodes']]\n",
        "\n",
        "    # Initialize generated commentary with the start token\n",
        "    generated_commentary = [tokenizer.encode('<start>')[0]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        com_input[0, :len(generated_commentary)] = generated_commentary\n",
        "        predictions = model.predict([tdat_input, com_input, node_input, edge_input], verbose=0)\n",
        "        next_token = np.argmax(predictions)\n",
        "        generated_commentary.append(next_token)\n",
        "        decoded_commentary = tokenizer.decode(generated_commentary)\n",
        "        if next_token == tokenizer.encode('<end>')[0]:\n",
        "            break\n",
        "\n",
        "    decoded_commentary = tokenizer.decode(generated_commentary)\n",
        "    return decoded_commentary\n"
      ],
      "metadata": {
        "id": "Mi8UiupSsvwP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "code='''\n",
        "factorial :: Integer -> Integer\n",
        "factorial 0 = 1\n",
        "factorial n = n * factorial (n - 1)\n",
        "\n",
        "'''\n",
        "tdat_encoded = tokenizer.encode(code)\n",
        "nodes, edges = extract_tree_features(code)\n",
        "num_nodes = min(len(nodes), int(config['maxastnodes']))\n",
        "node_encoded = []\n",
        "for node in nodes[:num_nodes]:\n",
        "    encoded_node = tokenizer.encode(node[0])\n",
        "    if len(encoded_node) > 1:\n",
        "      node_encoded.append(encoded_node[0])\n",
        "predict_commentary_greedy(model, tokenizer, tdat_encoded,node_encoded, edges, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6KlJmzlxvO5d",
        "outputId": "0b6efbc0-4649-4c7a-c691-1088669bd059"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> <end>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xo-UdK_thrb3",
        "outputId": "31d411a4-a51c-4408-8663-46b45d8747a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "absl-py==1.4.0\n",
            "accelerate==1.1.1\n",
            "aiohappyeyeballs==2.4.3\n",
            "aiohttp==3.11.1\n",
            "aiosignal==1.3.1\n",
            "alabaster==1.0.0\n",
            "albucore==0.0.19\n",
            "albumentations==1.4.20\n",
            "altair==4.2.2\n",
            "annotated-types==0.7.0\n",
            "anyio==3.7.1\n",
            "argon2-cffi==23.1.0\n",
            "argon2-cffi-bindings==21.2.0\n",
            "array_record==0.5.1\n",
            "arviz==0.20.0\n",
            "astropy==6.1.6\n",
            "astropy-iers-data==0.2024.11.11.0.32.38\n",
            "astunparse==1.6.3\n",
            "async-timeout==4.0.3\n",
            "atpublic==4.1.0\n",
            "attrs==24.2.0\n",
            "audioread==3.0.1\n",
            "autograd==1.7.0\n",
            "babel==2.16.0\n",
            "backcall==0.2.0\n",
            "beautifulsoup4==4.12.3\n",
            "bigframes==1.26.0\n",
            "bigquery-magics==0.4.0\n",
            "bleach==6.2.0\n",
            "blinker==1.9.0\n",
            "blis==0.7.11\n",
            "blosc2==2.7.1\n",
            "bokeh==3.6.1\n",
            "Bottleneck==1.4.2\n",
            "bqplot==0.12.43\n",
            "branca==0.8.0\n",
            "CacheControl==0.14.1\n",
            "cachetools==5.5.0\n",
            "catalogue==2.0.10\n",
            "certifi==2024.8.30\n",
            "cffi==1.17.1\n",
            "chardet==5.2.0\n",
            "charset-normalizer==3.4.0\n",
            "chex==0.1.87\n",
            "clarabel==0.9.0\n",
            "click==8.1.7\n",
            "cloudpathlib==0.20.0\n",
            "cloudpickle==3.1.0\n",
            "cmake==3.30.5\n",
            "cmdstanpy==1.2.4\n",
            "colorcet==3.1.0\n",
            "colorlover==0.3.0\n",
            "colour==0.1.5\n",
            "community==1.0.0b1\n",
            "confection==0.1.5\n",
            "cons==0.4.6\n",
            "contourpy==1.3.1\n",
            "cryptography==43.0.3\n",
            "cuda-python==12.2.1\n",
            "cudf-cu12 @ https://pypi.nvidia.com/cudf-cu12/cudf_cu12-24.10.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\n",
            "cufflinks==0.17.3\n",
            "cupy-cuda12x==12.2.0\n",
            "cvxopt==1.3.2\n",
            "cvxpy==1.5.3\n",
            "cycler==0.12.1\n",
            "cymem==2.0.8\n",
            "Cython==3.0.11\n",
            "dask==2024.10.0\n",
            "datascience==0.17.6\n",
            "db-dtypes==1.3.1\n",
            "dbus-python==1.2.18\n",
            "debugpy==1.8.0\n",
            "decorator==4.4.2\n",
            "defusedxml==0.7.1\n",
            "Deprecated==1.2.14\n",
            "diffusers==0.31.0\n",
            "distro==1.9.0\n",
            "dlib==19.24.2\n",
            "dm-tree==0.1.8\n",
            "docker-pycreds==0.4.0\n",
            "docstring_parser==0.16\n",
            "docutils==0.21.2\n",
            "dopamine_rl==4.0.9\n",
            "duckdb==1.1.3\n",
            "earthengine-api==1.2.0\n",
            "easydict==1.13\n",
            "ecos==2.0.14\n",
            "editdistance==0.8.1\n",
            "eerepr==0.0.4\n",
            "einops==0.8.0\n",
            "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl#sha256=86cc141f63942d4b2c5fcee06630fd6f904788d2f0ab005cce45aadb8fb73889\n",
            "entrypoints==0.4\n",
            "et_xmlfile==2.0.0\n",
            "etils==1.10.0\n",
            "etuples==0.3.9\n",
            "eval_type_backport==0.2.0\n",
            "exceptiongroup==1.2.2\n",
            "fastai==2.7.18\n",
            "fastcore==1.7.19\n",
            "fastdownload==0.0.7\n",
            "fastjsonschema==2.20.0\n",
            "fastprogress==1.0.3\n",
            "fastrlock==0.8.2\n",
            "filelock==3.16.1\n",
            "firebase-admin==6.5.0\n",
            "Flask==3.0.3\n",
            "flatbuffers==24.3.25\n",
            "flax==0.8.5\n",
            "folium==0.18.0\n",
            "fonttools==4.54.1\n",
            "frozendict==2.4.6\n",
            "frozenlist==1.5.0\n",
            "fsspec==2024.10.0\n",
            "future==1.0.0\n",
            "gast==0.6.0\n",
            "gcsfs==2024.10.0\n",
            "GDAL==3.6.4\n",
            "gdown==5.2.0\n",
            "geemap==0.35.1\n",
            "gensim==4.3.3\n",
            "geocoder==1.38.1\n",
            "geographiclib==2.0\n",
            "geopandas==1.0.1\n",
            "geopy==2.4.1\n",
            "gin-config==0.5.0\n",
            "gitdb==4.0.11\n",
            "GitPython==3.1.43\n",
            "glob2==0.7\n",
            "google==2.0.3\n",
            "google-ai-generativelanguage==0.6.10\n",
            "google-api-core==2.19.2\n",
            "google-api-python-client==2.151.0\n",
            "google-auth==2.27.0\n",
            "google-auth-httplib2==0.2.0\n",
            "google-auth-oauthlib==1.2.1\n",
            "google-cloud-aiplatform==1.71.1\n",
            "google-cloud-bigquery==3.25.0\n",
            "google-cloud-bigquery-connection==1.16.1\n",
            "google-cloud-bigquery-storage==2.27.0\n",
            "google-cloud-bigtable==2.27.0\n",
            "google-cloud-core==2.4.1\n",
            "google-cloud-datastore==2.20.1\n",
            "google-cloud-firestore==2.19.0\n",
            "google-cloud-functions==1.18.0\n",
            "google-cloud-iam==2.16.0\n",
            "google-cloud-language==2.15.0\n",
            "google-cloud-pubsub==2.27.1\n",
            "google-cloud-resource-manager==1.13.0\n",
            "google-cloud-storage==2.8.0\n",
            "google-cloud-translate==3.17.0\n",
            "google-colab @ file:///colabtools/dist/google_colab-1.0.0.tar.gz\n",
            "google-crc32c==1.6.0\n",
            "google-generativeai==0.8.3\n",
            "google-pasta==0.2.0\n",
            "google-resumable-media==2.7.2\n",
            "googleapis-common-protos==1.66.0\n",
            "googledrivedownloader==0.4\n",
            "graphviz==0.20.3\n",
            "greenlet==3.1.1\n",
            "grpc-google-iam-v1==0.13.1\n",
            "grpcio==1.67.1\n",
            "grpcio-status==1.62.3\n",
            "gspread==6.0.2\n",
            "gspread-dataframe==3.3.1\n",
            "gym==0.25.2\n",
            "gym-notices==0.0.8\n",
            "h11==0.14.0\n",
            "h5netcdf==1.4.1\n",
            "h5py==3.12.1\n",
            "holidays==0.60\n",
            "holoviews==1.20.0\n",
            "html5lib==1.1\n",
            "httpcore==1.0.6\n",
            "httpimport==1.4.0\n",
            "httplib2==0.22.0\n",
            "httpx==0.27.2\n",
            "huggingface-hub==0.26.2\n",
            "humanize==4.11.0\n",
            "hyperopt==0.2.7\n",
            "ibis-framework==9.2.0\n",
            "idna==3.10\n",
            "imageio==2.36.0\n",
            "imageio-ffmpeg==0.5.1\n",
            "imagesize==1.4.1\n",
            "imbalanced-learn==0.12.4\n",
            "imgaug==0.4.0\n",
            "immutabledict==4.2.0\n",
            "importlib_metadata==8.5.0\n",
            "importlib_resources==6.4.5\n",
            "imutils==0.5.4\n",
            "inflect==7.4.0\n",
            "iniconfig==2.0.0\n",
            "intel-cmplr-lib-ur==2025.0.0\n",
            "intel-openmp==2025.0.0\n",
            "ipyevents==2.0.2\n",
            "ipyfilechooser==0.6.0\n",
            "ipykernel==5.5.6\n",
            "ipyleaflet==0.19.2\n",
            "ipyparallel==8.8.0\n",
            "ipython==7.34.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.5.0\n",
            "ipytree==0.2.2\n",
            "ipywidgets==7.7.1\n",
            "itsdangerous==2.2.0\n",
            "jax==0.4.33\n",
            "jax-cuda12-pjrt==0.4.33\n",
            "jax-cuda12-plugin==0.4.33\n",
            "jaxlib==0.4.33\n",
            "jeepney==0.7.1\n",
            "jellyfish==1.1.0\n",
            "jieba==0.42.1\n",
            "Jinja2==3.1.4\n",
            "jiter==0.7.1\n",
            "joblib==1.4.2\n",
            "jsonpatch==1.33\n",
            "jsonpickle==4.0.0\n",
            "jsonpointer==3.0.0\n",
            "jsonschema==4.23.0\n",
            "jsonschema-specifications==2024.10.1\n",
            "jupyter-client==6.1.12\n",
            "jupyter-console==6.1.0\n",
            "jupyter-leaflet==0.19.2\n",
            "jupyter-server==1.24.0\n",
            "jupyter_core==5.7.2\n",
            "jupyterlab_pygments==0.3.0\n",
            "jupyterlab_widgets==3.0.13\n",
            "kaggle==1.6.17\n",
            "kagglehub==0.3.4\n",
            "keras==3.5.0\n",
            "keyring==23.5.0\n",
            "kiwisolver==1.4.7\n",
            "langchain==0.3.7\n",
            "langchain-core==0.3.18\n",
            "langchain-text-splitters==0.3.2\n",
            "langcodes==3.4.1\n",
            "langsmith==0.1.143\n",
            "language_data==1.2.0\n",
            "launchpadlib==1.10.16\n",
            "lazr.restfulclient==0.14.4\n",
            "lazr.uri==1.0.6\n",
            "lazy_loader==0.4\n",
            "libclang==18.1.1\n",
            "libcudf-cu12 @ https://pypi.nvidia.com/libcudf-cu12/libcudf_cu12-24.10.1-py3-none-manylinux_2_28_x86_64.whl\n",
            "librosa==0.10.2.post1\n",
            "lightgbm==4.5.0\n",
            "linkify-it-py==2.0.3\n",
            "llvmlite==0.43.0\n",
            "locket==1.0.0\n",
            "logical-unification==0.4.6\n",
            "lxml==5.3.0\n",
            "marisa-trie==1.2.1\n",
            "Markdown==3.7\n",
            "markdown-it-py==3.0.0\n",
            "MarkupSafe==3.0.2\n",
            "matplotlib==3.8.0\n",
            "matplotlib-inline==0.1.7\n",
            "matplotlib-venn==1.1.1\n",
            "mdit-py-plugins==0.4.2\n",
            "mdurl==0.1.2\n",
            "miniKanren==1.0.3\n",
            "missingno==0.5.2\n",
            "mistune==3.0.2\n",
            "mizani==0.13.0\n",
            "mkl==2025.0.0\n",
            "ml-dtypes==0.4.1\n",
            "mlxtend==0.23.2\n",
            "more-itertools==10.5.0\n",
            "moviepy==1.0.3\n",
            "mpmath==1.3.0\n",
            "msgpack==1.1.0\n",
            "multidict==6.1.0\n",
            "multipledispatch==1.0.0\n",
            "multitasking==0.0.11\n",
            "murmurhash==1.0.10\n",
            "music21==9.3.0\n",
            "namex==0.0.8\n",
            "natsort==8.4.0\n",
            "nbclassic==1.1.0\n",
            "nbclient==0.10.0\n",
            "nbconvert==7.16.4\n",
            "nbformat==5.10.4\n",
            "ndindex==1.9.2\n",
            "nest-asyncio==1.6.0\n",
            "networkx==3.4.2\n",
            "nibabel==5.3.2\n",
            "nltk==3.9.1\n",
            "notebook==6.5.5\n",
            "notebook_shim==0.2.4\n",
            "numba==0.60.0\n",
            "numexpr==2.10.1\n",
            "numpy==1.26.4\n",
            "nvidia-cublas-cu12==12.6.3.3\n",
            "nvidia-cuda-cupti-cu12==12.6.80\n",
            "nvidia-cuda-nvcc-cu12==12.6.77\n",
            "nvidia-cuda-runtime-cu12==12.6.77\n",
            "nvidia-cudnn-cu12==9.5.1.17\n",
            "nvidia-cufft-cu12==11.3.0.4\n",
            "nvidia-curand-cu12==10.3.7.77\n",
            "nvidia-cusolver-cu12==11.7.1.2\n",
            "nvidia-cusparse-cu12==12.5.4.2\n",
            "nvidia-nccl-cu12==2.23.4\n",
            "nvidia-nvjitlink-cu12==12.6.77\n",
            "nvtx==0.2.10\n",
            "nx-cugraph-cu12 @ https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.10.0-py3-none-any.whl\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.2.2\n",
            "openai==1.54.4\n",
            "opencv-contrib-python==4.10.0.84\n",
            "opencv-python==4.10.0.84\n",
            "opencv-python-headless==4.10.0.84\n",
            "openpyxl==3.1.5\n",
            "opentelemetry-api==1.28.1\n",
            "opentelemetry-sdk==1.28.1\n",
            "opentelemetry-semantic-conventions==0.49b1\n",
            "opt_einsum==3.4.0\n",
            "optax==0.2.4\n",
            "optree==0.13.1\n",
            "orbax-checkpoint==0.6.4\n",
            "orjson==3.10.11\n",
            "osqp==0.6.7.post3\n",
            "packaging==24.2\n",
            "pandas==2.2.2\n",
            "pandas-datareader==0.10.0\n",
            "pandas-gbq==0.24.0\n",
            "pandas-stubs==2.2.2.240909\n",
            "pandocfilters==1.5.1\n",
            "panel==1.5.4\n",
            "param==2.1.1\n",
            "parso==0.8.4\n",
            "parsy==2.1\n",
            "partd==1.4.2\n",
            "pathlib==1.0.1\n",
            "patsy==1.0.1\n",
            "peewee==3.17.8\n",
            "peft==0.13.2\n",
            "pexpect==4.9.0\n",
            "pickleshare==0.7.5\n",
            "pillow==11.0.0\n",
            "platformdirs==4.3.6\n",
            "plotly==5.24.1\n",
            "plotnine==0.14.1\n",
            "pluggy==1.5.0\n",
            "polars==1.9.0\n",
            "pooch==1.8.2\n",
            "portpicker==1.5.2\n",
            "preshed==3.0.9\n",
            "prettytable==3.12.0\n",
            "proglog==0.1.10\n",
            "progressbar2==4.5.0\n",
            "prometheus_client==0.21.0\n",
            "promise==2.3\n",
            "prompt_toolkit==3.0.48\n",
            "propcache==0.2.0\n",
            "prophet==1.1.6\n",
            "proto-plus==1.25.0\n",
            "protobuf==4.25.5\n",
            "psutil==5.9.5\n",
            "psycopg2==2.9.10\n",
            "ptyprocess==0.7.0\n",
            "py-cpuinfo==9.0.0\n",
            "py4j==0.10.9.7\n",
            "pyarrow==17.0.0\n",
            "pyarrow-hotfix==0.6\n",
            "pyasn1==0.6.1\n",
            "pyasn1_modules==0.4.1\n",
            "pycocotools==2.0.8\n",
            "pycparser==2.22\n",
            "pydantic==2.9.2\n",
            "pydantic_core==2.23.4\n",
            "pydata-google-auth==1.8.2\n",
            "pydot==3.0.2\n",
            "pydotplus==2.0.2\n",
            "PyDrive==1.3.1\n",
            "PyDrive2==1.21.1\n",
            "pyerfa==2.0.1.5\n",
            "pygame==2.6.1\n",
            "pygit2==1.16.0\n",
            "Pygments==2.18.0\n",
            "PyGObject==3.42.1\n",
            "PyJWT==2.9.0\n",
            "pylibcudf-cu12 @ https://pypi.nvidia.com/pylibcudf-cu12/pylibcudf_cu12-24.10.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\n",
            "pylibcugraph-cu12==24.10.0\n",
            "pylibraft-cu12==24.10.0\n",
            "pymc==5.18.0\n",
            "pymystem3==0.2.0\n",
            "pynvjitlink-cu12==0.4.0\n",
            "pyogrio==0.10.0\n",
            "PyOpenGL==3.1.7\n",
            "pyOpenSSL==24.2.1\n",
            "pyparsing==3.2.0\n",
            "pyperclip==1.9.0\n",
            "pyproj==3.7.0\n",
            "pyshp==2.3.1\n",
            "PySocks==1.7.1\n",
            "pyspark==3.5.3\n",
            "pytensor==2.25.5\n",
            "pytest==8.3.3\n",
            "python-apt==0.0.0\n",
            "python-box==7.2.0\n",
            "python-dateutil==2.8.2\n",
            "python-louvain==0.16\n",
            "python-slugify==8.0.4\n",
            "python-utils==3.9.0\n",
            "pytz==2024.2\n",
            "pyviz_comms==3.0.3\n",
            "PyYAML==6.0.2\n",
            "pyzmq==24.0.1\n",
            "qdldl==0.1.7.post4\n",
            "ratelim==0.1.6\n",
            "referencing==0.35.1\n",
            "regex==2024.9.11\n",
            "requests==2.32.3\n",
            "requests-oauthlib==1.3.1\n",
            "requests-toolbelt==1.0.0\n",
            "requirements-parser==0.9.0\n",
            "rich==13.9.4\n",
            "rmm-cu12==24.10.0\n",
            "rpds-py==0.21.0\n",
            "rpy2==3.4.2\n",
            "rsa==4.9\n",
            "safetensors==0.4.5\n",
            "scikit-image==0.24.0\n",
            "scikit-learn==1.5.2\n",
            "scipy==1.13.1\n",
            "scooby==0.10.0\n",
            "scs==3.2.7\n",
            "seaborn==0.13.2\n",
            "SecretStorage==3.3.1\n",
            "Send2Trash==1.8.3\n",
            "sentence-transformers==3.2.1\n",
            "sentencepiece==0.2.0\n",
            "sentry-sdk==2.18.0\n",
            "setproctitle==1.3.3\n",
            "shap==0.46.0\n",
            "shapely==2.0.6\n",
            "shellingham==1.5.4\n",
            "simple-parsing==0.1.6\n",
            "six==1.16.0\n",
            "sklearn-pandas==2.2.0\n",
            "slicer==0.0.8\n",
            "smart-open==7.0.5\n",
            "smmap==5.0.1\n",
            "sniffio==1.3.1\n",
            "snowballstemmer==2.2.0\n",
            "soundfile==0.12.1\n",
            "soupsieve==2.6\n",
            "soxr==0.5.0.post1\n",
            "spacy==3.7.5\n",
            "spacy-legacy==3.0.12\n",
            "spacy-loggers==1.0.5\n",
            "Sphinx==8.1.3\n",
            "sphinxcontrib-applehelp==2.0.0\n",
            "sphinxcontrib-devhelp==2.0.0\n",
            "sphinxcontrib-htmlhelp==2.1.0\n",
            "sphinxcontrib-jsmath==1.0.1\n",
            "sphinxcontrib-qthelp==2.0.0\n",
            "sphinxcontrib-serializinghtml==2.0.0\n",
            "SQLAlchemy==2.0.36\n",
            "sqlglot==25.1.0\n",
            "sqlparse==0.5.2\n",
            "srsly==2.4.8\n",
            "stanio==0.5.1\n",
            "statsmodels==0.14.4\n",
            "StrEnum==0.4.15\n",
            "stringzilla==3.10.10\n",
            "sympy==1.13.1\n",
            "tables==3.10.1\n",
            "tabulate==0.9.0\n",
            "tbb==2022.0.0\n",
            "tcmlib==1.2.0\n",
            "tenacity==9.0.0\n",
            "tensorboard==2.17.1\n",
            "tensorboard-data-server==0.7.2\n",
            "tensorflow==2.17.1\n",
            "tensorflow-datasets==4.9.7\n",
            "tensorflow-hub==0.16.1\n",
            "tensorflow-io-gcs-filesystem==0.37.1\n",
            "tensorflow-metadata==1.13.1\n",
            "tensorflow-probability==0.24.0\n",
            "tensorstore==0.1.68\n",
            "termcolor==2.5.0\n",
            "terminado==0.18.1\n",
            "text-unidecode==1.3\n",
            "textblob==0.17.1\n",
            "tf-slim==1.1.0\n",
            "tf_keras==2.17.0\n",
            "thinc==8.2.5\n",
            "threadpoolctl==3.5.0\n",
            "tifffile==2024.9.20\n",
            "timm==1.0.11\n",
            "tinycss2==1.4.0\n",
            "tokenizers==0.20.3\n",
            "toml==0.10.2\n",
            "tomli==2.1.0\n",
            "toolz==0.12.1\n",
            "torch @ https://download.pytorch.org/whl/cu121_full/torch-2.5.1%2Bcu121-cp310-cp310-linux_x86_64.whl\n",
            "torchaudio @ https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp310-cp310-linux_x86_64.whl\n",
            "torchsummary==1.5.1\n",
            "torchvision @ https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp310-cp310-linux_x86_64.whl\n",
            "tornado==6.3.3\n",
            "tqdm==4.66.6\n",
            "traitlets==5.7.1\n",
            "traittypes==0.2.1\n",
            "transformers==4.46.2\n",
            "tree-sitter==0.23.0\n",
            "tree-sitter-haskell==0.23.1\n",
            "tweepy==4.14.0\n",
            "typeguard==4.4.1\n",
            "typer==0.13.0\n",
            "types-pytz==2024.2.0.20241003\n",
            "types-setuptools==75.4.0.20241115\n",
            "typing_extensions==4.12.2\n",
            "tzdata==2024.2\n",
            "tzlocal==5.2\n",
            "uc-micro-py==1.0.3\n",
            "umf==0.9.0\n",
            "uritemplate==4.1.1\n",
            "urllib3==2.2.3\n",
            "vega-datasets==0.9.0\n",
            "wadllib==1.3.6\n",
            "wandb==0.18.7\n",
            "wasabi==1.1.3\n",
            "wcwidth==0.2.13\n",
            "weasel==0.4.1\n",
            "webcolors==24.11.1\n",
            "webencodings==0.5.1\n",
            "websocket-client==1.8.0\n",
            "Werkzeug==3.1.3\n",
            "widgetsnbextension==3.6.10\n",
            "wordcloud==1.9.4\n",
            "wrapt==1.16.0\n",
            "xarray==2024.10.0\n",
            "xarray-einstats==0.8.0\n",
            "xgboost==2.1.2\n",
            "xlrd==2.0.1\n",
            "xyzservices==2024.9.0\n",
            "yarl==1.17.1\n",
            "yellowbrick==1.5\n",
            "yfinance==0.2.49\n",
            "zipp==3.21.0\n"
          ]
        }
      ]
    }
  ]
}