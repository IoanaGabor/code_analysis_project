{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdhGW1MxQjnz",
        "outputId": "8a1bed31-fb7a-47c2-e1ca-e4d8f21507b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (1.9.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install polars\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "\n",
        "splits = {\n",
        "    'train': 'hf://datasets/blastwind/deprecated-github-code-haskell-function/data/train-*-of-*.parquet',\n",
        "    'test': 'hf://datasets/blastwind/deprecated-github-code-haskell-function/data/test-*-of-*.parquet',\n",
        "    'valid': 'hf://datasets/blastwind/deprecated-github-code-haskell-function/data/valid-00000-of-00001-636cb804972d8982.parquet'\n",
        "}\n",
        "\n",
        "\n",
        "df = pl.read_parquet(splits['train'])\n",
        "\n"
      ],
      "metadata": {
        "id": "3WsHzUqBVvnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataframe = df.to_pandas()\n",
        "\n",
        "dataframe = dataframe.loc[\n",
        "    (dataframe[\"is_commented\"] == True) & (dataframe[\"n_ast_nodes\"] < 50),\n",
        "    [\"full_code\", \"uncommented_code\", \"function_only_code\"]\n",
        "]\n",
        "\n",
        "\n",
        "dataframe[\"comments\"] = dataframe.apply(\n",
        "    lambda row: row[\"full_code\"].replace(row[\"uncommented_code\"], \"\").strip(), axis=1\n",
        ")\n",
        "\n",
        "dataframe = dataframe.head(10000)\n",
        "\n",
        "dataframe.to_csv('data-small.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "I14IsBN4ep0Z",
        "outputId": "cd924f10-6966-474f-fa08-b28d02a0c6bc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-337b54f295e0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m dataframe = dataframe.loc[\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"is_commented\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_ast_nodes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m\"full_code\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"uncommented_code\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"function_only_code\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "def tokenize_and_build_vocab(column):\n",
        "    vocab_counter = Counter()\n",
        "    for text in column:\n",
        "        tokens = [token.text for token in nlp(text.lower())]\n",
        "        vocab_counter.update(tokens)\n",
        "    return vocab_counter\n",
        "\n",
        "\n",
        "vocab_counter = tokenize_and_build_vocab(dataframe[\"comments\"])\n",
        "\n",
        "vocab = {token: idx for idx, (token, _) in enumerate(vocab_counter.items(), start=4)}  # Starting at index 4 for special tokens\n",
        "vocab['<unk>'] = 0\n",
        "vocab['<pad>'] = 1\n",
        "vocab['<bos>'] = 2\n",
        "vocab['<eos>'] = 3\n",
        "\n",
        "def text_to_ids(text, vocab):\n",
        "\n",
        "    tokens = [token.text for token in nlp(text.lower())]\n",
        "\n",
        "    return [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
        "\n",
        "text = dataframe[\"comments\"]\n",
        "token_ids = text_to_ids(text, vocab)\n",
        "\n",
        "print(f\"Text: {text}\")\n",
        "print(f\"Token IDs: {token_ids}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "SGNGJxukEXnU",
        "outputId": "e0706e6b-1087-46d5-85af-4d6522a94380"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-69743705a624>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# set library-specific custom warning handling before doing anything else\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup_default_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msetup_default_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/errors.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# fmt: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVARIABLE_RE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfigValidationError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPromise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_cupy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhas_cupy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/thinc/compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdlpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m \u001b[0;31m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2016\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_VF\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfunctional\u001b[0m  \u001b[0;31m# usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2017\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# usort: skip # noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_add_docstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# usort: skip # noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m from torch.nn import (\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mattention\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/attention/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_SDPBackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSDPBackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from torch.backends.cuda import (\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mcan_use_efficient_attention\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcan_use_flash_attention\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/backends/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m from torch.backends import (\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mcpu\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mcuda\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/backends/mps/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLibrary\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_Library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/library.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_library\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m from torch._library.custom_ops import (\n\u001b[1;32m     15\u001b[0m     \u001b[0m_maybe_get_opdef\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_library/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_library\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_library\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_impl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_library\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_registry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_library\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_library\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_class_registry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_fake_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "class CustomTokenizer:\n",
        "    def __init__(self, oov_token=\"<UNK>\"):\n",
        "        \"\"\"\n",
        "        Initializes the tokenizer with empty mappings and an OOV (out-of-vocabulary) token.\n",
        "        \"\"\"\n",
        "        self.word_to_id = {}\n",
        "        self.id_to_word = {}\n",
        "        self.oov_token = oov_token\n",
        "\n",
        "        self.word_to_id[oov_token] = 0\n",
        "        self.word_to_id[\"<start>\"] = 1;\n",
        "        self.word_to_id[\"<end>\"] = 2;\n",
        "        self.id_to_word[0] = oov_token\n",
        "        self.id_to_word[1] = \"<start>\"\n",
        "        self.id_to_word[2] = \"<end>\"\n",
        "\n",
        "    def load(self, word_list):\n",
        "        \"\"\"\n",
        "        Dynamically loads a list of words into the tokenizer and assigns free IDs.\n",
        "\n",
        "        Args:\n",
        "            word_list (list of str): The list of words to add to the tokenizer.\n",
        "        \"\"\"\n",
        "        # Ensure all words are lowercase\n",
        "        word_list = [word.lower() for word in word_list]\n",
        "\n",
        "        # Start assigning IDs from the current maximum ID + 1\n",
        "        next_id = max(self.word_to_id.values()) + 1\n",
        "\n",
        "        for word in word_list:\n",
        "            if word not in self.word_to_id:\n",
        "                self.word_to_id[word] = next_id\n",
        "                self.id_to_word[next_id] = word\n",
        "                next_id += 1\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"\n",
        "        Encodes a string into a list of IDs.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text to encode.\n",
        "\n",
        "        Returns:\n",
        "            list of int: List of word IDs.\n",
        "        \"\"\"\n",
        "        words = text.lower().split()  # Convert text to lowercase and split into words\n",
        "        return [self.word_to_id.get(word, self.word_to_id[self.oov_token]) for word in words]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"\n",
        "        Decodes a list of IDs into a string.\n",
        "\n",
        "        Args:\n",
        "            ids (list of int): List of word IDs to decode.\n",
        "\n",
        "        Returns:\n",
        "            str: The decoded string.\n",
        "        \"\"\"\n",
        "        return ' '.join(self.id_to_word.get(i, self.oov_token) for i in ids)\n",
        "\n",
        "    def export(self, filename):\n",
        "        \"\"\"\n",
        "        Exports the word_to_id and id_to_word mappings to a binary file.\n",
        "\n",
        "        Args:\n",
        "            filename (str): The path of the file to save the mappings.\n",
        "        \"\"\"\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump((self.word_to_id, self.id_to_word), f)\n",
        "\n",
        "    def import_mappings(self, filename):\n",
        "        \"\"\"\n",
        "        Loads word_to_id and id_to_word mappings from a binary file.\n",
        "\n",
        "        Args:\n",
        "            filename (str): The path of the file to load the mappings from.\n",
        "        \"\"\"\n",
        "        with open(filename, 'rb') as f:\n",
        "            self.word_to_id, self.id_to_word = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "xnMqL9j_UZLp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "rdGhKxbng1-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "ZzhICbXemOKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataframe = pd.read_csv('data-small.csv')\n",
        "tokenizer = CustomTokenizer()\n",
        "dataframe = dataframe.head(10)\n",
        "tokenizer = CustomTokenizer()\n",
        "for comment in dataframe[\"comments\"]:\n",
        "  tokenizer.load(comment.lower().split())"
      ],
      "metadata": {
        "id": "QUjiBGukUDF0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98_gRPuEhzTI",
        "outputId": "acce1004-06a4-418a-f9b5-1b25cd0b2706"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tree_sitter==0.23.0\n",
        "!pip install tree_sitter_haskell"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5QUCsAKihb-",
        "outputId": "57d4afa0-d2ac-4e5e-c75c-d458f0352d57"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tree_sitter==0.23.0\n",
            "  Downloading tree_sitter-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Downloading tree_sitter-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (558 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/558.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m450.6/558.7 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.7/558.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tree_sitter\n",
            "Successfully installed tree_sitter-0.23.0\n",
            "Collecting tree_sitter_haskell\n",
            "  Downloading tree_sitter_haskell-0.23.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading tree_sitter_haskell-0.23.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (424 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.6/424.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tree_sitter_haskell\n",
            "Successfully installed tree_sitter_haskell-0.23.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tree_sitter_haskell as tshaskell\n",
        "import numpy as np\n",
        "from tree_sitter import Language, Parser\n",
        "\n",
        "\n",
        "HS_LANGUAGE = Language(tshaskell.language())\n",
        "parser = Parser(HS_LANGUAGE)\n"
      ],
      "metadata": {
        "id": "XNf9tk_JjQRH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_tree_features(code):\n",
        "    try:\n",
        "        example_bytes = code.encode()\n",
        "        tree = parser.parse(example_bytes)\n",
        "        root_node = tree.root_node\n",
        "\n",
        "        def extract_features(node, parent_index=None, nodes=[], edges=[]):\n",
        "            node_type = node.type\n",
        "            start_pos = node.start_point\n",
        "            end_pos = node.end_point\n",
        "            code_value = code[node.start_byte:node.end_byte]\n",
        "            tokenizer.load(node_type.lower().split())\n",
        "            feature_vector = [node_type]\n",
        "            node_index = len(nodes)\n",
        "            nodes.append(feature_vector)\n",
        "\n",
        "            if parent_index is not None:\n",
        "                edges.append((parent_index, node_index))\n",
        "\n",
        "            for child in node.children:\n",
        "                extract_features(child, parent_index=node_index, nodes=nodes, edges=edges)\n",
        "\n",
        "            return nodes, edges\n",
        "\n",
        "        nodes, edge_list = extract_features(root_node)\n",
        "        code_value = code[root_node.start_byte:root_node.end_byte]\n",
        "        tokenizer.load(code_value.lower().split())\n",
        "        num_nodes = len(nodes)\n",
        "        adj_matrix = np.zeros((num_nodes, num_nodes), dtype=np.float32)\n",
        "        for parent, child in edge_list:\n",
        "            adj_matrix[parent, child] = 1\n",
        "\n",
        "        return nodes, adj_matrix\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing code: {e}\")\n",
        "        return None, None\n",
        "dataframe[\"nodes\"], dataframe[\"adjacency_matrix\"] = zip(*dataframe[\"function_only_code\"].apply(extract_tree_features))\n"
      ],
      "metadata": {
        "id": "g9xsJdoSjmAA"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataframe.to_csv('data-ast-small.csv', index=False)\n",
        "#print(dataframe[\"nodes\"].size)\n",
        "#tokenizer.export(\"tokens.bin\")\n",
        "import pandas as pd\n",
        "dataframe = pd.read_csv('data-ast-small.csv')"
      ],
      "metadata": {
        "id": "bxAQoO4GqcsO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EfB7okPrNBxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smvAVY58KWAl",
        "outputId": "9651c681-3a37-46ab-94b8-e25914d1db8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXGNXmBEtjy_",
        "outputId": "8cbc2076-8920-414c-ec14-0015701d82a9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.67.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Layer\n",
        "from keras import activations\n",
        "import keras.backend as K\n",
        "\n",
        "class GCNLayer(Layer):\n",
        "    def __init__(self, units, activation='relu', initializer='glorot_uniform', sparse=False, use_bias=True, **kwargs):\n",
        "        self.activation = activations.get(activation)\n",
        "        self.output_dim = units\n",
        "        self.initializer = initializer\n",
        "        self.sparse = sparse\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        super(GCNLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(name='kernel',\n",
        "                                          shape=(input_shape[0][-1], self.output_dim),\n",
        "                                          initializer=self.initializer,\n",
        "                                          trainable=True)\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(name='bias',\n",
        "                                              shape=(self.output_dim,),\n",
        "                                              initializer='zeros',\n",
        "                                              trainable=True)\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "        super(GCNLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        assert isinstance(x, list)\n",
        "        nodes, edges = x\n",
        "        identity = tf.eye(tf.shape(edges)[-1], batch_shape=[tf.shape(edges)[0]], dtype=edges.dtype)\n",
        "        edges = edges + identity\n",
        "        output = tf.matmul(edges,nodes)\n",
        "        output = tf.matmul(output, self.kernel)\n",
        "\n",
        "        if self.use_bias:\n",
        "            output += self.bias\n",
        "\n",
        "        return self.activation(output)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        return (None,input_shape[0][1], self.output_dim)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'units': self.output_dim,\n",
        "            'activation': activations.serialize(self.activation),\n",
        "        }\n",
        "\n",
        "        base_config = super(GCNLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "metadata": {
        "id": "qImh_MqWthex"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import keras.utils\n",
        "from keras.layers import Input, Dense, Embedding, Activation, concatenate, Flatten, GRU, TimeDistributed, dot\n",
        "from keras.models import Model\n",
        "\n",
        "class CodeGNNGRU:\n",
        "    def __init__(self, config):\n",
        "        config['modeltype'] = 'codegnngru'\n",
        "\n",
        "        self.config = config\n",
        "        self.tdatvocabsize = config['tdatvocabsize']\n",
        "        self.comvocabsize = config['comvocabsize']\n",
        "        self.smlvocabsize = config['smlvocabsize']\n",
        "        self.tdatlen = config['tdatlen']\n",
        "        self.comlen = config['comlen']\n",
        "        self.smllen = config['maxastnodes']\n",
        "\n",
        "        self.config['batch_maker'] = 'graph_multi_1'\n",
        "\n",
        "        self.embdims = 100\n",
        "        self.smldims = 256\n",
        "        self.recdims = 256\n",
        "        self.tdddims = 256\n",
        "\n",
        "    def create_model(self):\n",
        "\n",
        "        tdat_input = Input(shape=(self.tdatlen,))\n",
        "        com_input = Input(shape=(self.comlen,))\n",
        "        node_input = Input(shape=(self.smllen,))\n",
        "        edge_input = Input(shape=(self.smllen, self.smllen))\n",
        "\n",
        "        tdel = Embedding(output_dim=self.embdims, input_dim=self.tdatvocabsize, mask_zero=False)\n",
        "        tde = tdel(tdat_input)\n",
        "\n",
        "        se = tdel(node_input)\n",
        "\n",
        "        tenc = GRU(self.recdims, return_state=True, return_sequences=True)\n",
        "        tencout, tstate_h = tenc(tde)\n",
        "\n",
        "        de = Embedding(output_dim=self.embdims, input_dim=self.comvocabsize, mask_zero=False)(com_input)\n",
        "        dec = GRU(self.recdims, return_sequences=True)\n",
        "        decout = dec(de, initial_state=tstate_h)\n",
        "\n",
        "        tattn = dot([decout, tencout], axes=[2, 2])\n",
        "        tattn = Activation('softmax')(tattn)\n",
        "        tcontext = dot([tattn, tencout], axes=[2, 1])\n",
        "\n",
        "        astwork = se\n",
        "\n",
        "        for i in range(self.config['asthops']):\n",
        "            astwork = GCNLayer(100)([astwork, edge_input])\n",
        "\n",
        "        astwork = GRU(self.recdims, return_sequences=True)(astwork, initial_state=tstate_h)\n",
        "\n",
        "        aattn = dot([decout, astwork], axes=[2, 2])\n",
        "        aattn = Activation('softmax')(aattn)\n",
        "        acontext = dot([aattn, astwork], axes=[2, 1])\n",
        "\n",
        "        context = concatenate([tcontext, decout, acontext])\n",
        "\n",
        "        out = TimeDistributed(Dense(self.tdddims, activation=\"relu\"))(context)\n",
        "\n",
        "        out = Flatten()(out)\n",
        "        out1 = Dense(self.comvocabsize, activation=\"softmax\")(out)\n",
        "\n",
        "        model = Model(inputs=[tdat_input, com_input, node_input, edge_input], outputs=out1)\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])\n",
        "        return self.config, model\n"
      ],
      "metadata": {
        "id": "BWrWF8f6txTV"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.utils import to_categorical\n",
        "import ast\n",
        "\n",
        "\n",
        "config = {\n",
        "    'tdatvocabsize': 1000,\n",
        "    'comvocabsize': 1000,\n",
        "    'smlvocabsize':1000,\n",
        "    'tdatlen': 200,\n",
        "    'comlen': 500,\n",
        "    'maxastnodes': 70,\n",
        "    'asthops': 3,\n",
        "}\n",
        "model_instance = CodeGNNGRU(config)\n",
        "config, model = model_instance.create_model()\n",
        "\n",
        "tdat_input = np.zeros((len(dataframe) * config['comlen'], config['tdatlen']), dtype=np.float32)\n",
        "com_input = np.zeros((len(dataframe) * config['comlen'], config['comlen']), dtype=np.float32)\n",
        "node_input = np.zeros((len(dataframe) * config['comlen'], config['maxastnodes']), dtype=np.float32)\n",
        "edge_input = np.zeros((len(dataframe) * config['comlen'], config['maxastnodes'], config['maxastnodes']), dtype=np.float32)\n",
        "target_output = np.zeros((len(dataframe) * config['comlen'], config['comvocabsize']), dtype=np.float32)\n",
        "dataframe = dataframe.head(10)\n",
        "sample_idx = 0\n",
        "\n",
        "for i, (nodes, adj_matrix, tdat_text, com_text) in enumerate(zip(dataframe[\"nodes\"], dataframe[\"adjacency_matrix\"], dataframe[\"function_only_code\"],dataframe[\"comments\"])):\n",
        "    tdat_encoded = tokenizer.encode(tdat_text)\n",
        "    com_encoded = [1] + tokenizer.encode(com_text) + [2]\n",
        "    num_nodes = min(len(nodes), int(config['maxastnodes']))\n",
        "    node_encoded = []\n",
        "    for node in nodes[:num_nodes]:\n",
        "        encoded_node = tokenizer.encode(node[0])\n",
        "        if len(encoded_node) > 1:\n",
        "          node_encoded.append(encoded_node[0])\n",
        "\n",
        "    for j in range(len(com_encoded)):\n",
        "        comout = keras.utils.to_categorical(com_encoded[j], num_classes=config['comvocabsize'])\n",
        "        tdat_input[sample_idx, :len(tdat_encoded)] = tdat_encoded[:config[\"tdatlen\"]]\n",
        "        com_input[sample_idx, :j] = com_encoded[:j]\n",
        "        target_output[sample_idx, :len(comout)] = comout\n",
        "        node_input[sample_idx, :len(node_encoded)] = node_encoded[:num_nodes]\n",
        "        var =  adj_matrix[:num_nodes, :num_nodes]\n",
        "        edge_input[sample_idx, :num_nodes, :num_nodes] = var\n",
        "        sample_idx +=1\n",
        "\n",
        "tdat_input = tdat_input[:sample_idx]\n",
        "com_input = com_input[:sample_idx]\n",
        "node_input = node_input[:sample_idx]\n",
        "edge_input = edge_input[:sample_idx]\n",
        "target_output = target_output[:sample_idx]\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "tdat_input = tf.convert_to_tensor(tdat_input, dtype=tf.float32)\n",
        "com_input = tf.convert_to_tensor(com_input, dtype=tf.float32)\n",
        "node_input = tf.convert_to_tensor(node_input,dtype=tf.float32)\n",
        "edge_input = tf.convert_to_tensor(edge_input,dtype=tf.float32)\n",
        "target_output = tf.convert_to_tensor(target_output,dtype=tf.float32)\n",
        "\n",
        "model.fit(\n",
        "    [tdat_input, com_input, node_input, edge_input],\n",
        "    target_output,\n",
        "    epochs=5\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "dGMm76R9uLfQ",
        "outputId": "1eafd31b-13f6-4b74-ef48-5b5e0fd7e7c2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OperatorNotAllowedInGraphError",
          "evalue": "Exception encountered when calling GRU.call().\n\n\u001b[1mIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by GRU.call():\n  • sequences=tf.Tensor(shape=(None, 200, 100), dtype=float32)\n  • initial_state=None\n  • mask=None\n  • training=True",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-6d2e8c2a5a9d>\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mtarget_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mtdat_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcom_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mtarget_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: Exception encountered when calling GRU.call().\n\n\u001b[1mIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by GRU.call():\n  • sequences=tf.Tensor(shape=(None, 200, 100), dtype=float32)\n  • initial_state=None\n  • mask=None\n  • training=True"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = np.sum(target_output[:, 2] == 1)\n",
        "\n",
        "print(f\"Number of rows with 1 in the second position: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hMrzNYiKb-c",
        "outputId": "ad5ef587-36e6-4e2c-d6a8-b05159a8f83d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows with 1 in the second position: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('code_gnn_model.keras')\n",
        "print(\"Model saved to 'code_gnn_model.h5'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09t1NKlSvSZF",
        "outputId": "a31e265d-4ebf-4e1d-c9d2-2a9b3452e77e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to 'code_gnn_model.h5'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_commentary_greedy(model, tokenizer, tdat_encoded, node_encoded, edge_matrix, config, max_length=20):\n",
        "    \"\"\"\n",
        "    Predict commentary for given input in a greedy manner.\n",
        "\n",
        "    Args:\n",
        "        model: The trained CodeGNNGRU model.\n",
        "        tokenizer: Tokenizer to encode/decode text.\n",
        "        tdat_encoded: Encoded representation of the function code.\n",
        "        node_encoded: Encoded representation of the AST nodes.\n",
        "        edge_matrix: Adjacency matrix of the AST.\n",
        "        config: Configuration dictionary containing lengths and vocab sizes.\n",
        "        max_length: Maximum length of the generated commentary.\n",
        "\n",
        "    Returns:\n",
        "        Generated commentary string.\n",
        "    \"\"\"\n",
        "    # Initialize inputs\n",
        "    tdat_input = np.zeros((1, config['tdatlen']), dtype=np.float32)\n",
        "    node_input = np.zeros((1, config['maxastnodes']), dtype=np.float32)\n",
        "    edge_input = np.zeros((1, config['maxastnodes'], config['maxastnodes']), dtype=np.float32)\n",
        "    com_input = np.zeros((1, config['comlen']), dtype=np.float32)\n",
        "\n",
        "    # Populate input tensors\n",
        "    tdat_input[0, :len(tdat_encoded)] = tdat_encoded[:config['tdatlen']]\n",
        "    node_input[0, :len(node_encoded)] = node_encoded[:config['maxastnodes']]\n",
        "    edge_input[0, :len(edge_matrix), :len(edge_matrix)] = edge_matrix[:config['maxastnodes'], :config['maxastnodes']]\n",
        "\n",
        "    # Initialize generated commentary with the start token\n",
        "    generated_commentary = [tokenizer.encode('<start>')[0]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        com_input[0, :len(generated_commentary)] = generated_commentary\n",
        "        predictions = model.predict([tdat_input, com_input, node_input, edge_input], verbose=0)\n",
        "        next_token = np.argmax(predictions)\n",
        "        generated_commentary.append(next_token)\n",
        "        decoded_commentary = tokenizer.decode(generated_commentary)\n",
        "        print(decoded_commentary)\n",
        "        if next_token == tokenizer.encode('<end>')[0]:\n",
        "            break\n",
        "\n",
        "    decoded_commentary = tokenizer.decode(generated_commentary)\n",
        "    return decoded_commentary\n"
      ],
      "metadata": {
        "id": "Mi8UiupSsvwP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "code='''\n",
        "showb = BSL.pack . show\n",
        "'''\n",
        "tdat_encoded = tokenizer.encode(code)\n",
        "nodes, edges = extract_tree_features(code)\n",
        "num_nodes = min(len(nodes), int(config['maxastnodes']))\n",
        "node_encoded = []\n",
        "for node in nodes[:num_nodes]:\n",
        "    encoded_node = tokenizer.encode(node[0])\n",
        "    if len(encoded_node) > 1:\n",
        "      node_encoded.append(encoded_node[0])\n",
        "predict_commentary_greedy(model, tokenizer, tdat_encoded,node_encoded, edges, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6KlJmzlxvO5d",
        "outputId": "fd1186da-0f12-4112-f68f-ccc268205b61"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> <end>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> <end>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.head()\n",
        "print(target_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iVHbnBzI67-",
        "outputId": "c7accf70-8deb-4dc8-9c01-009952de14f9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    }
  ]
}