{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 2.62kB/s]\n",
      "config.json: 100%|██████████| 385/385 [00:00<00:00, 32.9kB/s]\n",
      "vocab.txt: 100%|██████████| 397k/397k [00:00<00:00, 1.78MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 500M/500M [00:55<00:00, 8.99MB/s] \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdumitrescustefan/bert-base-romanian-cased-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# tokenize a sentence and run through the model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mtensor(tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcesta este un test.\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Batch size 1\u001b[39;00m\n\u001b[1;32m      7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# get encoding\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer, AutoTokenizer, AutoModel\n",
    "# load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
    "model = AutoModel.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
    "# tokenize a sentence and run through the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ioana/anaconda3/envs/codetf/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
    "model = AutoModel.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
    "\n",
    "\n",
    "class SatireDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = self.data.iloc[idx]['title']\n",
    "        content = self.data.iloc[idx]['content']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        \n",
    "        # Tokenize both title and content\n",
    "        encoding_title = self.tokenizer(title, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        encoding_content = self.tokenizer(content, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        \n",
    "        return {\n",
    "            'input_ids_title': encoding_title['input_ids'].squeeze(0),\n",
    "            'attention_mask_title': encoding_title['attention_mask'].squeeze(0),\n",
    "            'input_ids_content': encoding_content['input_ids'].squeeze(0),\n",
    "            'attention_mask_content': encoding_content['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Define the Siamese Network\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.bert = model  # Pre-trained BERT model\n",
    "        self.fc = nn.Linear(768, 1)  # Binary classification (0 or 1)\n",
    "\n",
    "    def forward(self, input_ids_1, attention_mask_1, input_ids_2, attention_mask_2):\n",
    "        # Get BERT embeddings for both inputs\n",
    "        output_1 = self.bert(input_ids_1, attention_mask=attention_mask_1)\n",
    "        output_2 = self.bert(input_ids_2, attention_mask=attention_mask_2)\n",
    "        \n",
    "        # Use the [CLS] token's output (first token's hidden state)\n",
    "        emb_1 = output_1.last_hidden_state[:, 0, :]  # First token (CLS) embedding for the title\n",
    "        emb_2 = output_2.last_hidden_state[:, 0, :]  # First token (CLS) embedding for the content\n",
    "        \n",
    "        # Compute cosine similarity between the two embeddings\n",
    "        cosine_sim = torch.nn.functional.cosine_similarity(emb_1, emb_2)\n",
    "        \n",
    "        # Output prediction using a fully connected layer\n",
    "        output = self.fc(cosine_sim.unsqueeze(1))  # [batch_size, 1]\n",
    "        \n",
    "        return torch.sigmoid(output)  # Sigmoid activation for binary classification\n",
    "\n",
    "# Initialize dataset and dataloaders\n",
    "dataset = SatireDataset(df, tokenizer)\n",
    "train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "siamese_model = SiameseNetwork(model)\n",
    "siamese_model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary cross entropy loss\n",
    "optimizer = optim.Adam(siamese_model.parameters(), lr=1e-5)\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 3\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    siamese_model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        # Move data to GPU if available\n",
    "        input_ids_title = batch['input_ids_title'].to(device)\n",
    "        attention_mask_title = batch['attention_mask_title'].to(device)\n",
    "        input_ids_content = batch['input_ids_content'].to(device)\n",
    "        attention_mask_content = batch['attention_mask_content'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = siamese_model(input_ids_title, attention_mask_title, input_ids_content, attention_mask_content)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(output.squeeze(1), labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track loss and predictions\n",
    "        running_loss += loss.item()\n",
    "        all_preds.extend(output.squeeze().cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute training accuracy and loss for this epoch\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    all_preds = np.array(all_preds) > 0.5  # Convert to binary predictions (0 or 1)\n",
    "    all_labels = np.array(all_labels)\n",
    "    epoch_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy*100:.2f}%\")\n",
    "    \n",
    "# Save the trained model\n",
    "torch.save(siamese_model.state_dict(), \"siamese_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4894, -0.6289,  0.4323,  ...,  0.5619,  0.1866,  0.0627],\n",
      "         [ 1.3910, -0.1289, -0.6514,  ...,  1.0870, -1.3180, -0.4085],\n",
      "         [ 0.4701,  0.0223,  0.8850,  ...,  1.4155, -0.0656, -0.5369],\n",
      "         ...,\n",
      "         [-0.0217, -0.7029,  0.0935,  ...,  1.1168,  0.4694, -0.6496],\n",
      "         [ 0.5665, -0.0897,  0.4867,  ...,  0.4900,  0.6538, -0.4164],\n",
      "         [ 0.9089, -0.3590,  0.4672,  ...,  1.0321,  0.7096, -0.5094]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "input_ids = torch.tensor(tokenizer.encode(\"Acesta este un test.\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(input_ids)\n",
    "# get encoding\n",
    "last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "print(last_hidden_states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codetf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
